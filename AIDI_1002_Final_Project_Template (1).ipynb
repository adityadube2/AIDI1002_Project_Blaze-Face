{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: AIDI 1002 Final Term Project Report\n",
    "\n",
    "#### Members' Names or Individual's Name:\n",
    "<ol><li>Aditya Dube</li>\n",
    "<li>Shimoni Mistry</li></ol>\n",
    "                                 \n",
    "\n",
    "####  Emails:\n",
    "   <li> 200530940@student.georgianc.on.ca</li>\n",
    "   <li> 200523189@student.georgianc.on.ca</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: \n",
    "#### BlazeFace is a real-time face detection model developed by Google, which uses a lightweight architecture to achieve fast inference on mobile devices and low-power computers.\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "The problem addressed by BlazeFace is real-time face detection with high accuracy while using minimal computational resources.\n",
    "\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "Real-time face detection is crucial in many applications such as video conferencing, security systems, and augmented reality. However, existing face detection models may not be optimized for mobile devices or low-power computers, making real-time face detection a challenging task.\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Prior approaches to real-time face detection have been limited by either their computational complexity or their accuracy. Traditional methods based on sliding windows require significant computational resources and may not work well under varying lighting and pose conditions.\n",
    "#### Solution:\n",
    "\n",
    "BlazeFace addresses these limitations by using a lightweight architecture based on the Single Shot Detector (SSD) architecture and a modified version of the MobileNetV1 architecture for feature extraction. This enables fast and accurate face detection on mobile devices and low-power computers with minimal computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Tom et al. [1] | They trained a BERT based transformer to predict answers from the passage of a question| SQUAD dataset for QA | Only 80% accuracy\n",
    "| George et al. [2] | They trained a attention based sequence to sequence model using LSTM to predict answers from the passage of a question| SQUAD V2 dataset for QA | High accuracy but poor on unkown answers\n",
    "| Weiss et al. [3] |They proposed a real-time object detection model using YOLOv2 and achieved high accuracy |PASCAL VOC dataset Image dataset for object detection | Not optimized for mobile devices\n",
    "|BlazeFace (discussed in this paper) |A real-time face detection model using a lightweight architecture based on SSD and a modified version of MobileNetV1 for feature extraction. |WIDER FACE and AFW datasets |face detection Requires large training data for high accuracy. Future work can involve further optimization for real-time performance on low-power devices.\n",
    "\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The BlazeFace model is a lightweight real-time face detection model that uses a modified version of the MobileNetV1 architecture for feature extraction and the Single Shot Detector (SSD) architecture for object detection.\n",
    "\n",
    "The model consists of a feature extraction network followed by two parallel branches that predict the location and class of face bounding boxes. The feature extraction network is composed of a series of depthwise separable convolutions, which significantly reduces the computational cost of the model.\n",
    "\n",
    "The first branch predicts the location of the face bounding box by regressing the coordinates of the top-left and bottom-right corners of the box. The second branch predicts the probability that the detected object is a face.\n",
    "\n",
    "During training, the model is optimized using a combination of smooth L1 loss for bounding box regression and focal loss for classification. The model is trained on large-scale face detection datasets, such as the WIDER FACE and AFW datasets.\n",
    "\n",
    "BlazeFace achieves state-of-the-art accuracy on the WIDER FACE and AFW benchmarks while using minimal computational resources. The model is designed to run efficiently on mobile devices, with a small model size and low computational requirements.\n",
    "\n",
    "In this section, we will be implementing the BlazeFace model for real-time face detection in images and videos. We will also explore ways to optimize the model for even faster inference on low-power devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_options = {\n",
    "    \"num_layers\": 4,\n",
    "    \"min_scale\": 0.1484375,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"input_size_height\": 128,\n",
    "    \"input_size_width\": 128,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [8, 16, 16, 16],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_back_options = {\n",
    "    \"num_layers\": 4,\n",
    "    \"min_scale\": 0.15625,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"input_size_height\": 256,\n",
    "    \"input_size_width\": 256,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [16, 32, 32, 32],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_back_options = {\n",
    "    \"num_layers\": 4,\n",
    "    \"min_scale\": 0.15625,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"input_size_height\": 256,\n",
    "    \"input_size_width\": 256,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [16, 32, 32, 32],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scale(min_scale, max_scale, stride_index, num_strides):\n",
    "    return min_scale + (max_scale - min_scale) * stride_index / (num_strides - 1.0)\n",
    "\n",
    "\n",
    "def generate_anchors(options):\n",
    "    strides_size = len(options[\"strides\"])\n",
    "    assert options[\"num_layers\"] == strides_size\n",
    "\n",
    "    anchors = []\n",
    "    layer_id = 0\n",
    "    while layer_id < strides_size:\n",
    "        anchor_height = []\n",
    "        anchor_width = []\n",
    "        aspect_ratios = []\n",
    "        scales = []\n",
    "\n",
    "        # For same strides, we merge the anchors in the same order.\n",
    "        last_same_stride_layer = layer_id\n",
    "        while (last_same_stride_layer < strides_size) and \\\n",
    "              (options[\"strides\"][last_same_stride_layer] == options[\"strides\"][layer_id]):\n",
    "            scale = calculate_scale(options[\"min_scale\"],\n",
    "                                    options[\"max_scale\"],\n",
    "                                    last_same_stride_layer,\n",
    "                                    strides_size)\n",
    "\n",
    "            if last_same_stride_layer == 0 and options[\"reduce_boxes_in_lowest_layer\"]:\n",
    "                # For first layer, it can be specified to use predefined anchors.\n",
    "                aspect_ratios.append(1.0)\n",
    "                aspect_ratios.append(2.0)\n",
    "                aspect_ratios.append(0.5)\n",
    "                scales.append(0.1)\n",
    "                scales.append(scale)\n",
    "                scales.append(scale)                \n",
    "            else:\n",
    "                for aspect_ratio in options[\"aspect_ratios\"]:\n",
    "                    aspect_ratios.append(aspect_ratio)\n",
    "                    scales.append(scale)\n",
    "\n",
    "                if options[\"interpolated_scale_aspect_ratio\"] > 0.0:\n",
    "                    scale_next = 1.0 if last_same_stride_layer == strides_size - 1 \\\n",
    "                                     else calculate_scale(options[\"min_scale\"],\n",
    "                                                          options[\"max_scale\"],\n",
    "                                                          last_same_stride_layer + 1,\n",
    "                                                          strides_size)\n",
    "                    scales.append(np.sqrt(scale * scale_next))\n",
    "                    aspect_ratios.append(options[\"interpolated_scale_aspect_ratio\"])\n",
    "\n",
    "            last_same_stride_layer += 1\n",
    "\n",
    "        for i in range(len(aspect_ratios)):\n",
    "            ratio_sqrts = np.sqrt(aspect_ratios[i])\n",
    "            anchor_height.append(scales[i] / ratio_sqrts)\n",
    "            anchor_width.append(scales[i] * ratio_sqrts)            \n",
    "            \n",
    "        stride = options[\"strides\"][layer_id]\n",
    "        feature_map_height = int(np.ceil(options[\"input_size_height\"] / stride))\n",
    "        feature_map_width = int(np.ceil(options[\"input_size_width\"] / stride))\n",
    "\n",
    "        for y in range(feature_map_height):\n",
    "            for x in range(feature_map_width):\n",
    "                for anchor_id in range(len(anchor_height)):\n",
    "                    x_center = (x + options[\"anchor_offset_x\"]) / feature_map_width\n",
    "                    y_center = (y + options[\"anchor_offset_y\"]) / feature_map_height\n",
    "\n",
    "                    new_anchor = [x_center, y_center, 0, 0]\n",
    "                    if options[\"fixed_anchor_size\"]:\n",
    "                        new_anchor[2] = 1.0\n",
    "                        new_anchor[3] = 1.0\n",
    "                    else:\n",
    "                        new_anchor[2] = anchor_width[anchor_id]\n",
    "                        new_anchor[3] = anchor_height[anchor_id]\n",
    "                    anchors.append(new_anchor)\n",
    "\n",
    "        layer_id = last_same_stride_layer\n",
    "\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = generate_anchors(anchor_options)\n",
    "\n",
    "assert len(anchors) == 896\n",
    "\n",
    "anchors_back = generate_anchors(anchor_back_options)\n",
    "\n",
    "assert len(anchors_back) == 896\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03125, 0.03125, 1.0, 1.0],\n",
       " [0.03125, 0.03125, 1.0, 1.0],\n",
       " [0.09375, 0.03125, 1.0, 1.0],\n",
       " [0.09375, 0.03125, 1.0, 1.0],\n",
       " [0.15625, 0.03125, 1.0, 1.0],\n",
       " [0.15625, 0.03125, 1.0, 1.0],\n",
       " [0.21875, 0.03125, 1.0, 1.0],\n",
       " [0.21875, 0.03125, 1.0, 1.0],\n",
       " [0.28125, 0.03125, 1.0, 1.0],\n",
       " [0.28125, 0.03125, 1.0, 1.0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03125, 0.03125, 1.0, 1.0],\n",
       " [0.03125, 0.03125, 1.0, 1.0],\n",
       " [0.09375, 0.03125, 1.0, 1.0],\n",
       " [0.09375, 0.03125, 1.0, 1.0],\n",
       " [0.15625, 0.03125, 1.0, 1.0],\n",
       " [0.15625, 0.03125, 1.0, 1.0],\n",
       " [0.21875, 0.03125, 1.0, 1.0],\n",
       " [0.21875, 0.03125, 1.0, 1.0],\n",
       " [0.28125, 0.03125, 1.0, 1.0],\n",
       " [0.28125, 0.03125, 1.0, 1.0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors_back[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 0\n"
     ]
    }
   ],
   "source": [
    "anchor_options_test = {\n",
    "    \"num_layers\": 5,\n",
    "    \"min_scale\": 0.1171875,\n",
    "    \"max_scale\": 0.75,\n",
    "    \"input_size_height\": 256,\n",
    "    \"input_size_width\": 256,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [8, 16, 32, 32, 32],\n",
    "    \"aspect_ratios\": [1.0],\n",
    "    \"reduce_boxes_in_lowest_layer\": False,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": True,\n",
    "}\n",
    "\n",
    "anchors_test = generate_anchors(anchor_options_test)\n",
    "anchors_golden = np.loadtxt(\"anchor_golden_file_0.txt\")\n",
    "\n",
    "assert len(anchors_test) == len(anchors_golden)\n",
    "print(\"Number of errors:\", (np.abs(anchors_test - anchors_golden) > 1e-5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 0\n"
     ]
    }
   ],
   "source": [
    "anchor_options_test = {\n",
    "    \"num_layers\": 6,\n",
    "    \"min_scale\": 0.2,\n",
    "    \"max_scale\": 0.95,\n",
    "    \"input_size_height\": 300,\n",
    "    \"input_size_width\": 300,\n",
    "    \"anchor_offset_x\": 0.5,\n",
    "    \"anchor_offset_y\": 0.5,\n",
    "    \"strides\": [16, 32, 64, 128, 256, 512],\n",
    "    \"aspect_ratios\": [1.0, 2.0, 0.5, 3.0, 0.3333],\n",
    "    \"reduce_boxes_in_lowest_layer\": True,\n",
    "    \"interpolated_scale_aspect_ratio\": 1.0,\n",
    "    \"fixed_anchor_size\": False,\n",
    "}\n",
    "\n",
    "anchors_test = generate_anchors(anchor_options_test)\n",
    "anchors_golden = np.loadtxt(\"anchor_golden_file_1.txt\")\n",
    "\n",
    "assert len(anchors_test) == len(anchors_golden)\n",
    "print(\"Number of errors:\", (np.abs(anchors_test - anchors_golden) > 1e-5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anchors.npy\", anchors)\n",
    "np.save(\"anchorsback.npy\", anchors_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/hollance/BlazeFace-PyTorch.git\n",
      "  Cloning https://github.com/hollance/BlazeFace-PyTorch.git to c:\\users\\aditya\\appdata\\local\\temp\\pip-req-build-4lyci31r\n",
      "  Resolved https://github.com/hollance/BlazeFace-PyTorch.git to commit 852bfd8e3d44ed6775761105bdcead4ef389a538\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/hollance/BlazeFace-PyTorch.git 'C:\\Users\\Aditya\\AppData\\Local\\Temp\\pip-req-build-4lyci31r'\n",
      "ERROR: git+https://github.com/hollance/BlazeFace-PyTorch.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/hollance/BlazeFace-PyTorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_detections(img, detections, with_keypoints=True):\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.grid(False)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    if isinstance(detections, torch.Tensor):\n",
    "        detections = detections.cpu().numpy()\n",
    "\n",
    "    if detections.ndim == 1:\n",
    "        detections = np.expand_dims(detections, axis=0)\n",
    "\n",
    "    print(\"Found %d faces\" % detections.shape[0])\n",
    "        \n",
    "    for i in range(detections.shape[0]):\n",
    "        ymin = detections[i, 0] * img.shape[0]\n",
    "        xmin = detections[i, 1] * img.shape[1]\n",
    "        ymax = detections[i, 2] * img.shape[0]\n",
    "        xmax = detections[i, 3] * img.shape[1]\n",
    "\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                 linewidth=1, edgecolor=\"r\", facecolor=\"none\", \n",
    "                                 alpha=detections[i, 16])\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        if with_keypoints:\n",
    "            for k in range(6):\n",
    "                kp_x = detections[i, 4 + k*2    ] * img.shape[1]\n",
    "                kp_y = detections[i, 4 + k*2 + 1] * img.shape[0]\n",
    "                circle = patches.Circle((kp_x, kp_y), radius=0.5, linewidth=1, \n",
    "                                        edgecolor=\"lightskyblue\", facecolor=\"none\", \n",
    "                                        alpha=detections[i, 16])\n",
    "                ax.add_patch(circle)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from blazeface import BlazeFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BlazeFace model\n",
    "face_detector = BlazeFace()\n",
    "face_detector.load_weights(\"blazeface.pth\")\n",
    "face_detector.load_anchors(\"anchors.npy\")\n",
    "\n",
    "# Set the video source\n",
    "video_path = \"path/to/your/video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Read the frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB and resize\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    \n",
    "    # Detect faces\n",
    "    try:\n",
    "        faces = face_detector.predict_on_image(frame)\n",
    "    except BlazeFaceError:\n",
    "        continue\n",
    "    \n",
    "    # Draw bounding boxes around the detected faces\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.astype(int)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Read the frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB and resize\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    \n",
    "    # Detect faces\n",
    "    try:\n",
    "        faces = face_detector.predict_on_image(frame)\n",
    "    except BlazeFaceError:\n",
    "        continue\n",
    "    \n",
    "    # Draw bounding boxes around the detected faces\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.astype(int)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "\n",
    "front_net = BlazeFace().to(gpu)\n",
    "front_net.load_weights(\"blazeface.pth\")\n",
    "front_net.load_anchors(\"anchors.npy\")\n",
    "back_net = BlazeFace(back_model=True).to(gpu)\n",
    "back_net.load_weights(\"blazefaceback.pth\")\n",
    "back_net.load_anchors(\"anchorsback.npy\")\n",
    "\n",
    "# Optionally change the thresholds:\n",
    "front_net.min_score_thresh = 0.75\n",
    "front_net.min_suppression_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"1face.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_detections = front_net.predict_on_image(img)\n",
    "front_detections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(img, front_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(img2, back_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [ \"1face.jpg\", \"3faces.png\", \"4faces.png\" ]\n",
    "\n",
    "xfront = np.zeros((len(filenames), 128, 128, 3), dtype=np.uint8)\n",
    "xback = np.zeros((len(filenames), 256, 256, 3), dtype=np.uint8)\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    xfront[i] = img\n",
    "    xback[i] = cv2.resize(img, (256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_detections = front_net.predict_on_batch(xfront)\n",
    "[d.shape for d in front_detections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xfront[0], front_detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xfront[1], front_detections[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xfront[2], front_detections[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_detections = back_net.predict_on_batch(xback)\n",
    "[d.shape for d in back_detections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xback[0], back_detections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xback[1], back_detections[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detections(xback[2], back_detections[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added face detection in video\n",
    "    Video is divided into frames and then converted to RGB and passed on to predict_on_image() method of the BlazeFace object. If a face is detected, a rectangle is drawn around it. The code exits when the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from blazeface import BlazeFace\n",
    "\n",
    "# Load the BlazeFace detector\n",
    "detector = BlazeFace()\n",
    "\n",
    "# Load the video stream\n",
    "cap = cv2.VideoCapture(scr=\"video1.avi\")\n",
    "\n",
    "# Check if the video stream was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream\")\n",
    "    exit()\n",
    "\n",
    "# Loop over the frames from the video stream\n",
    "while True:\n",
    "    # Read the frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was not read successfully, break from the loop\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        # Detect faces in the frame using BlazeFace\n",
    "        faces = detector.predict_on_image(rgb)\n",
    "\n",
    "        # Draw a rectangle around each detected face\n",
    "        for face in faces:\n",
    "            x, y, w, h = face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    except BlazeFaceError:\n",
    "        pass\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "We tried to add Face Detection in video and got successful results for some video formats and video size (mp4 and avi seemed work the most).\n",
    "We also tried to change the image size to 256x256px in the blazeface.py file, got better results but the processing time was increased and the model used more resources and was not able to detect faces in miliseconds as earlier.\n",
    "<br><br>\n",
    "We also tried implementing MediaPipe Holistic model to detect not only the face but also the pose, left and right hands, and landmarks of the detected objects but weren't successful as it's accuracy was not as expected and also crashed the notebbok a couple of time so we had to remove it.\n",
    "\n",
    "During this project, We learned how to use BlazeFace for face detection in images and videos using Python and OpenCV. I also gained knowledge on how to fine-tune the detection results by adjusting the confidence threshold and the non-maximum suppression overlap threshold.\n",
    " <br>The results showed that BlazeFace is a robust and efficient face detection model, capable of detecting multiple faces in real-time videos with high accuracy. However, the model is limited to detecting only frontal faces and may not perform well under challenging conditions, such as occlusion, extreme lighting, and blurry images.\n",
    "\n",
    "In the future, the limitations of BlazeFace can be addressed by incorporating more complex and powerful deep learning models or by combining multiple models for better performance. Additionally, further research can be done on adapting face detection models for non-frontal views and other facial features such as facial expressions and emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Chen Chen, Haibin Ling, Yanfeng Sun, and Wei Xia. \"BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs.\" 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020, pp. 619-620.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
